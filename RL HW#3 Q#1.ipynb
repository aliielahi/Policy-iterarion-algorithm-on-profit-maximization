{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL HW #3 Question #1: Rental Car Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amalearn.reward import RewardBase\n",
    "from amalearn.environment import EnvironmentBase\n",
    "from amalearn.agent import AgentBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CARS = 20\n",
    "MAX_MOVE_OF_CARS = 5\n",
    "EXPECTED_FIRST_LOC_REQUESTS = 3\n",
    "EXPECTED_SECOND_LOC_REQUESTS = 4\n",
    "EXPECTED_FIRST_LOC_RETURNS = 3\n",
    "EXPECTED_SECOND_LOC_RETURNS = 2\n",
    "DISCOUNT_RATE = 0.9\n",
    "RENTAL_CREDIT = 10\n",
    "COST_OF_MOVING = 2\n",
    "POISSON_UPPER_BOUND = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReward(RewardBase):\n",
    "    def __init__(self):\n",
    "        super(MyReward, self).__init__()\n",
    "\n",
    "    def get_reward(self, Return, Transition):\n",
    "        return 10 * Return - 2 * Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myEnvironment(EnvironmentBase):\n",
    "    def __init__(self, _states, _actions, _rewards, episode_max_length, id, container=None):\n",
    "        state_space = gym.spaces.Discrete(len(_states))\n",
    "        action_space = gym.spaces.Discrete(len(_actions))\n",
    "        super(myEnvironment, self).__init__(action_space, state_space, id, container)\n",
    "        self.rewards = _rewards\n",
    "        self.states = _states\n",
    "        self.actions = _actions\n",
    "        self.episode_max_length = episode_max_length\n",
    "        self.state = {\n",
    "            'length': 0,\n",
    "            'current': None,\n",
    "            'last_action': None\n",
    "        }\n",
    "\n",
    "    def calculate_reward(self, action):\n",
    "        return self.rewards.get_reward(Return, Transition)\n",
    "    \n",
    "    def terminated(self):\n",
    "        return self.state['length'] >= self.episode_max_length\n",
    "\n",
    "    def observe(self):\n",
    "        return self.state['current']\n",
    "\n",
    "    def available_actions(self):\n",
    "        return self.action_space.n\n",
    "\n",
    "    def next_state(self, action):\n",
    "        self.state['length'] += 1\n",
    "        self.state['last_action'] = action\n",
    "\n",
    "    def reset(self):\n",
    "        self.state['length'] = 0\n",
    "        self.state['last_action'] = None\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print('{}:\\taction={}'.format(self.state['length'], self.state['last_action']))\n",
    "\n",
    "    def close(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAgent(AgentBase):\n",
    "    def __init__(self, id, environment, init_state):\n",
    "        super(eGreedyAgent, self).__init__(id, environment)\n",
    "        self.trial = 0\n",
    "        self.current_state = init_state\n",
    "        self.policy = np.zeros()\n",
    "        self.stateVal = np.zeros((MAX_CARS + 1, MAX_CARS + 1))\n",
    "    \n",
    "    def take_action(self) -> (object, float, bool, object):\n",
    "        \n",
    "        self.trial += 1\n",
    "        \n",
    "        available_actions = self.environment.available_actions()\n",
    "        \n",
    "        # choosing an action\n",
    "        \n",
    "        next_state, reward, done, info = self.environment.step(action)\n",
    "        \n",
    "        # updating values + policy improvement\n",
    "\n",
    "        if done: print('-----Terminated-----')\n",
    "        \n",
    "        return state, reward, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.zeros((MAX_CARS + 1, MAX_CARS + 1))\n",
    "stateVal = np.zeros((MAX_CARS + 1, MAX_CARS + 1))\n",
    "states = [[i, j] for i in range(MAX_CARS + 1) for j in range(MAX_CARS + 1)]\n",
    "actions = np.arange(-MAX_MOVE_OF_CARS, MAX_MOVE_OF_CARS + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = MyReward()\n",
    "environment = myQ2Environment(rewards, 11, 100000, '1')\n",
    "myAgent = MyAgent('1', 11, environment)\n",
    "\n",
    "result = []\n",
    "for step in range(100000):\n",
    "    result.append(myAgent.take_action())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
